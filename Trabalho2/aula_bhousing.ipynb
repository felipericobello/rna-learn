{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import scipy as sp\n",
    "import tensorflow\n",
    "import keras\n",
    "import tensorflow.keras\n",
    "import sklearn as sk\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]\n",
      "Tensor Flow Version: 2.0.0\n",
      "Tensorflow Keras Version: 2.2.4-tf\n",
      "Keras Version: 2.3.1\n",
      "Scikit-Learn 0.24.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Tensor Flow Version: {tensorflow.__version__}\")\n",
    "print(f\"Tensorflow Keras Version: {tensorflow.keras.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,), (102, 13), (102,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.random.seed(1)\n",
    "tensorflow.random.set_seed(1)\n",
    "\n",
    "# Import dos dados de treinamento\n",
    "dataset = tensorflow.keras.datasets.boston_housing\n",
    "(train_data, train_labels), (test_data, test_labels) = dataset.load_data(seed=1)\n",
    "train_data.shape, train_labels.shape, test_data.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.00000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.595577</td>\n",
       "      <td>11.591584</td>\n",
       "      <td>11.035322</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>0.554059</td>\n",
       "      <td>6.310631</td>\n",
       "      <td>68.482426</td>\n",
       "      <td>3.779059</td>\n",
       "      <td>9.623762</td>\n",
       "      <td>409.25495</td>\n",
       "      <td>18.496782</td>\n",
       "      <td>354.071931</td>\n",
       "      <td>12.448812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.524506</td>\n",
       "      <td>23.676616</td>\n",
       "      <td>6.813216</td>\n",
       "      <td>0.254290</td>\n",
       "      <td>0.114267</td>\n",
       "      <td>0.698453</td>\n",
       "      <td>28.114483</td>\n",
       "      <td>2.094428</td>\n",
       "      <td>8.761354</td>\n",
       "      <td>168.28937</td>\n",
       "      <td>2.135036</td>\n",
       "      <td>95.241643</td>\n",
       "      <td>7.123851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.00000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.081348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>5.897500</td>\n",
       "      <td>43.250000</td>\n",
       "      <td>2.106075</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.00000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>374.052500</td>\n",
       "      <td>6.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.250895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>6.243500</td>\n",
       "      <td>76.950000</td>\n",
       "      <td>3.167500</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>334.00000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>391.190000</td>\n",
       "      <td>10.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.681942</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.630250</td>\n",
       "      <td>94.100000</td>\n",
       "      <td>5.116700</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.00000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>395.962500</td>\n",
       "      <td>16.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.00000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean     3.595577   11.591584   11.035322    0.069307    0.554059    6.310631   \n",
       "std      8.524506   23.676616    6.813216    0.254290    0.114267    0.698453   \n",
       "min      0.006320    0.000000    0.740000    0.000000    0.385000    3.561000   \n",
       "25%      0.081348    0.000000    5.130000    0.000000    0.453000    5.897500   \n",
       "50%      0.250895    0.000000    8.560000    0.000000    0.535000    6.243500   \n",
       "75%      3.681942   12.500000   18.100000    0.000000    0.624000    6.630250   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD        TAX     PTRATIO           B  \\\n",
       "count  404.000000  404.000000  404.000000  404.00000  404.000000  404.000000   \n",
       "mean    68.482426    3.779059    9.623762  409.25495   18.496782  354.071931   \n",
       "std     28.114483    2.094428    8.761354  168.28937    2.135036   95.241643   \n",
       "min      2.900000    1.129600    1.000000  187.00000   12.600000    0.320000   \n",
       "25%     43.250000    2.106075    4.000000  279.00000   17.400000  374.052500   \n",
       "50%     76.950000    3.167500    5.000000  334.00000   19.100000  391.190000   \n",
       "75%     94.100000    5.116700   24.000000  666.00000   20.200000  395.962500   \n",
       "max    100.000000   12.126500   24.000000  711.00000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT  \n",
       "count  404.000000  \n",
       "mean    12.448812  \n",
       "std      7.123851  \n",
       "min      1.730000  \n",
       "25%      6.745000  \n",
       "50%     10.805000  \n",
       "75%     16.672500  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criação de atributos para os inputs da rede\n",
    "atrib = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "dataframe = pd.DataFrame(train_data, columns=atrib)\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.416523</td>\n",
       "      <td>0.905322</td>\n",
       "      <td>-1.301339</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-0.719025</td>\n",
       "      <td>0.771758</td>\n",
       "      <td>0.064729</td>\n",
       "      <td>-0.285089</td>\n",
       "      <td>-0.299841</td>\n",
       "      <td>-1.114076</td>\n",
       "      <td>-0.045387</td>\n",
       "      <td>0.450236</td>\n",
       "      <td>-0.691327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.419329</td>\n",
       "      <td>1.835661</td>\n",
       "      <td>-1.066210</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-0.613878</td>\n",
       "      <td>0.552431</td>\n",
       "      <td>-0.430291</td>\n",
       "      <td>0.933650</td>\n",
       "      <td>-0.528400</td>\n",
       "      <td>-0.233548</td>\n",
       "      <td>-0.420552</td>\n",
       "      <td>0.450236</td>\n",
       "      <td>-0.740519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.395387</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>-0.606239</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-0.929320</td>\n",
       "      <td>-0.402288</td>\n",
       "      <td>0.606046</td>\n",
       "      <td>0.913237</td>\n",
       "      <td>-0.756958</td>\n",
       "      <td>-1.048631</td>\n",
       "      <td>-0.279865</td>\n",
       "      <td>0.406503</td>\n",
       "      <td>0.892644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.415517</td>\n",
       "      <td>0.038415</td>\n",
       "      <td>-0.729682</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-1.271048</td>\n",
       "      <td>-0.620182</td>\n",
       "      <td>-1.676745</td>\n",
       "      <td>1.299788</td>\n",
       "      <td>-0.642679</td>\n",
       "      <td>-0.382286</td>\n",
       "      <td>0.189092</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>-0.611215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009707</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>1.038194</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>1.892133</td>\n",
       "      <td>-1.359874</td>\n",
       "      <td>0.987105</td>\n",
       "      <td>-0.800952</td>\n",
       "      <td>1.642904</td>\n",
       "      <td>1.527508</td>\n",
       "      <td>0.798736</td>\n",
       "      <td>0.280877</td>\n",
       "      <td>-0.317471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.243648</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>1.038194</td>\n",
       "      <td>3.664502</td>\n",
       "      <td>0.674177</td>\n",
       "      <td>0.533795</td>\n",
       "      <td>1.008473</td>\n",
       "      <td>-1.158007</td>\n",
       "      <td>1.642904</td>\n",
       "      <td>1.527508</td>\n",
       "      <td>0.798736</td>\n",
       "      <td>0.223478</td>\n",
       "      <td>-1.225408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>-0.400987</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>-0.537170</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-0.535017</td>\n",
       "      <td>0.093707</td>\n",
       "      <td>-0.505078</td>\n",
       "      <td>0.363958</td>\n",
       "      <td>-0.528400</td>\n",
       "      <td>-0.727358</td>\n",
       "      <td>0.517362</td>\n",
       "      <td>0.450236</td>\n",
       "      <td>-0.784088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>-0.410561</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>-1.260191</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-0.578829</td>\n",
       "      <td>0.361774</td>\n",
       "      <td>0.965737</td>\n",
       "      <td>-0.445570</td>\n",
       "      <td>-0.756958</td>\n",
       "      <td>-1.286612</td>\n",
       "      <td>-0.326761</td>\n",
       "      <td>0.450236</td>\n",
       "      <td>-0.951340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>-0.246567</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>1.255688</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>2.777123</td>\n",
       "      <td>-1.299667</td>\n",
       "      <td>1.122434</td>\n",
       "      <td>-1.045713</td>\n",
       "      <td>-0.528400</td>\n",
       "      <td>-0.037214</td>\n",
       "      <td>-1.780528</td>\n",
       "      <td>-0.131113</td>\n",
       "      <td>0.116821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.488217</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>1.038194</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>1.392684</td>\n",
       "      <td>-0.013806</td>\n",
       "      <td>0.541943</td>\n",
       "      <td>-0.476118</td>\n",
       "      <td>1.642904</td>\n",
       "      <td>1.527508</td>\n",
       "      <td>0.798736</td>\n",
       "      <td>-0.860584</td>\n",
       "      <td>0.531437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0   -0.416523  0.905322 -1.301339 -0.272888 -0.719025  0.771758  0.064729   \n",
       "1   -0.419329  1.835661 -1.066210 -0.272888 -0.613878  0.552431 -0.430291   \n",
       "2   -0.395387 -0.490186 -0.606239 -0.272888 -0.929320 -0.402288  0.606046   \n",
       "3   -0.415517  0.038415 -0.729682 -0.272888 -1.271048 -0.620182 -1.676745   \n",
       "4    0.009707 -0.490186  1.038194 -0.272888  1.892133 -1.359874  0.987105   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "399  0.243648 -0.490186  1.038194  3.664502  0.674177  0.533795  1.008473   \n",
       "400 -0.400987 -0.490186 -0.537170 -0.272888 -0.535017  0.093707 -0.505078   \n",
       "401 -0.410561 -0.490186 -1.260191 -0.272888 -0.578829  0.361774  0.965737   \n",
       "402 -0.246567 -0.490186  1.255688 -0.272888  2.777123 -1.299667  1.122434   \n",
       "403  0.488217 -0.490186  1.038194 -0.272888  1.392684 -0.013806  0.541943   \n",
       "\n",
       "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0   -0.285089 -0.299841 -1.114076 -0.045387  0.450236 -0.691327  \n",
       "1    0.933650 -0.528400 -0.233548 -0.420552  0.450236 -0.740519  \n",
       "2    0.913237 -0.756958 -1.048631 -0.279865  0.406503  0.892644  \n",
       "3    1.299788 -0.642679 -0.382286  0.189092  0.442982 -0.611215  \n",
       "4   -0.800952  1.642904  1.527508  0.798736  0.280877 -0.317471  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "399 -1.158007  1.642904  1.527508  0.798736  0.223478 -1.225408  \n",
       "400  0.363958 -0.528400 -0.727358  0.517362  0.450236 -0.784088  \n",
       "401 -0.445570 -0.756958 -1.286612 -0.326761  0.450236 -0.951340  \n",
       "402 -1.045713 -0.528400 -0.037214 -1.780528 -0.131113  0.116821  \n",
       "403 -0.476118  1.642904  1.527508  0.798736 -0.860584  0.531437  \n",
       "\n",
       "[404 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pré-processamento dos dados\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "\n",
    "train_data = (train_data - mean) / std\n",
    "test_data =  (test_data  - mean)  / std\n",
    "dataframe = pd.DataFrame(train_data, columns=atrib)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.411433</td>\n",
       "      <td>0.947610</td>\n",
       "      <td>-0.726742</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-1.060754</td>\n",
       "      <td>0.264295</td>\n",
       "      <td>-1.783583</td>\n",
       "      <td>0.818727</td>\n",
       "      <td>-0.299841</td>\n",
       "      <td>-0.477478</td>\n",
       "      <td>-1.123988</td>\n",
       "      <td>0.310523</td>\n",
       "      <td>-0.531103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.411665</td>\n",
       "      <td>1.412780</td>\n",
       "      <td>-1.116175</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-1.025705</td>\n",
       "      <td>0.917976</td>\n",
       "      <td>-1.673183</td>\n",
       "      <td>1.291087</td>\n",
       "      <td>-0.528400</td>\n",
       "      <td>-0.066961</td>\n",
       "      <td>-1.546049</td>\n",
       "      <td>0.248183</td>\n",
       "      <td>-1.032858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.413042</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>0.263738</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-1.025705</td>\n",
       "      <td>-0.053945</td>\n",
       "      <td>-2.225184</td>\n",
       "      <td>0.225850</td>\n",
       "      <td>-0.528400</td>\n",
       "      <td>-0.066961</td>\n",
       "      <td>0.095300</td>\n",
       "      <td>0.429421</td>\n",
       "      <td>-0.796738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.420553</td>\n",
       "      <td>2.892864</td>\n",
       "      <td>-1.326321</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-1.043229</td>\n",
       "      <td>0.464987</td>\n",
       "      <td>-1.381157</td>\n",
       "      <td>2.182266</td>\n",
       "      <td>-0.642679</td>\n",
       "      <td>-0.769004</td>\n",
       "      <td>-0.701927</td>\n",
       "      <td>0.387580</td>\n",
       "      <td>-0.907770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.407332</td>\n",
       "      <td>0.778458</td>\n",
       "      <td>-0.897211</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-1.104565</td>\n",
       "      <td>0.118077</td>\n",
       "      <td>-2.161081</td>\n",
       "      <td>1.556739</td>\n",
       "      <td>-0.414120</td>\n",
       "      <td>-0.650014</td>\n",
       "      <td>-0.889509</td>\n",
       "      <td>0.216960</td>\n",
       "      <td>-1.020208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.418149</td>\n",
       "      <td>2.892864</td>\n",
       "      <td>-1.086784</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-1.420007</td>\n",
       "      <td>-0.623049</td>\n",
       "      <td>-1.758654</td>\n",
       "      <td>2.601181</td>\n",
       "      <td>-0.985516</td>\n",
       "      <td>-0.560771</td>\n",
       "      <td>-0.983301</td>\n",
       "      <td>0.432154</td>\n",
       "      <td>-0.449585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.411552</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>-0.033112</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-1.235999</td>\n",
       "      <td>-0.352115</td>\n",
       "      <td>-2.161081</td>\n",
       "      <td>0.721013</td>\n",
       "      <td>-0.642679</td>\n",
       "      <td>-0.620266</td>\n",
       "      <td>0.329779</td>\n",
       "      <td>0.387265</td>\n",
       "      <td>-0.973828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.267382</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>1.038194</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>1.217438</td>\n",
       "      <td>0.135279</td>\n",
       "      <td>0.979982</td>\n",
       "      <td>-1.004983</td>\n",
       "      <td>1.642904</td>\n",
       "      <td>1.527508</td>\n",
       "      <td>0.798736</td>\n",
       "      <td>0.450236</td>\n",
       "      <td>0.972756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.383503</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>-0.710577</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-0.412345</td>\n",
       "      <td>-0.322011</td>\n",
       "      <td>-0.248665</td>\n",
       "      <td>-0.060788</td>\n",
       "      <td>-0.185562</td>\n",
       "      <td>-0.608367</td>\n",
       "      <td>-0.514344</td>\n",
       "      <td>0.238406</td>\n",
       "      <td>-0.220493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-0.412903</td>\n",
       "      <td>-0.490186</td>\n",
       "      <td>-0.745847</td>\n",
       "      <td>-0.272888</td>\n",
       "      <td>-0.482444</td>\n",
       "      <td>-0.660320</td>\n",
       "      <td>-0.960924</td>\n",
       "      <td>0.074165</td>\n",
       "      <td>-0.528400</td>\n",
       "      <td>-0.774954</td>\n",
       "      <td>0.329779</td>\n",
       "      <td>0.450236</td>\n",
       "      <td>-0.517048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0   -0.411433  0.947610 -0.726742 -0.272888 -1.060754  0.264295 -1.783583   \n",
       "1   -0.411665  1.412780 -1.116175 -0.272888 -1.025705  0.917976 -1.673183   \n",
       "2   -0.413042 -0.490186  0.263738 -0.272888 -1.025705 -0.053945 -2.225184   \n",
       "3   -0.420553  2.892864 -1.326321 -0.272888 -1.043229  0.464987 -1.381157   \n",
       "4   -0.407332  0.778458 -0.897211 -0.272888 -1.104565  0.118077 -2.161081   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "97  -0.418149  2.892864 -1.086784 -0.272888 -1.420007 -0.623049 -1.758654   \n",
       "98  -0.411552 -0.490186 -0.033112 -0.272888 -1.235999 -0.352115 -2.161081   \n",
       "99   0.267382 -0.490186  1.038194 -0.272888  1.217438  0.135279  0.979982   \n",
       "100 -0.383503 -0.490186 -0.710577 -0.272888 -0.412345 -0.322011 -0.248665   \n",
       "101 -0.412903 -0.490186 -0.745847 -0.272888 -0.482444 -0.660320 -0.960924   \n",
       "\n",
       "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0    0.818727 -0.299841 -0.477478 -1.123988  0.310523 -0.531103  \n",
       "1    1.291087 -0.528400 -0.066961 -1.546049  0.248183 -1.032858  \n",
       "2    0.225850 -0.528400 -0.066961  0.095300  0.429421 -0.796738  \n",
       "3    2.182266 -0.642679 -0.769004 -0.701927  0.387580 -0.907770  \n",
       "4    1.556739 -0.414120 -0.650014 -0.889509  0.216960 -1.020208  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "97   2.601181 -0.985516 -0.560771 -0.983301  0.432154 -0.449585  \n",
       "98   0.721013 -0.642679 -0.620266  0.329779  0.387265 -0.973828  \n",
       "99  -1.004983  1.642904  1.527508  0.798736  0.450236  0.972756  \n",
       "100 -0.060788 -0.185562 -0.608367 -0.514344  0.238406 -0.220493  \n",
       "101  0.074165 -0.528400 -0.774954  0.329779  0.450236 -0.517048  \n",
       "\n",
       "[102 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataframe = pd.DataFrame(test_data, columns=atrib)\n",
    "test_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rna\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                448       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,537\n",
      "Trainable params: 1,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definição do modelo\n",
    "model = Sequential([Dense(32, activation=tensorflow.nn.relu, input_shape=(train_data.shape[1],)), Dense(32, activation=tensorflow.nn.relu), Dense(1)], name='rna')\n",
    "model.compile(optimizer=tensorflow.keras.optimizers.RMSprop(learning_rate=0.002), loss='mean_squared_error', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323 samples, validate on 81 samples\n",
      "Epoch 1/600\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001E3142BFE58> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001E3142BFE58> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 556.8198 - mae: 21.7279 - val_loss: 532.3686 - val_mae: 21.0049\n",
      "Epoch 2/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 483.2234 - mae: 19.9660 - val_loss: 446.6012 - val_mae: 18.9500\n",
      "Epoch 3/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 397.3378 - mae: 17.7855 - val_loss: 350.3325 - val_mae: 16.3672\n",
      "Epoch 4/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 299.3157 - mae: 15.0088 - val_loss: 235.9257 - val_mae: 12.7955\n",
      "Epoch 5/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 195.1884 - mae: 11.5944 - val_loss: 148.3257 - val_mae: 9.2043\n",
      "Epoch 6/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 119.7827 - mae: 8.5570 - val_loss: 99.5681 - val_mae: 7.4039\n",
      "Epoch 7/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 77.9423 - mae: 6.6532 - val_loss: 81.4673 - val_mae: 6.4911\n",
      "Epoch 8/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 59.2505 - mae: 5.7285 - val_loss: 68.5142 - val_mae: 5.6986\n",
      "Epoch 9/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 48.3577 - mae: 5.0550 - val_loss: 57.8518 - val_mae: 5.0611\n",
      "Epoch 10/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 39.0932 - mae: 4.5169 - val_loss: 50.6554 - val_mae: 4.6368\n",
      "Epoch 11/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 33.5433 - mae: 4.1780 - val_loss: 45.6847 - val_mae: 4.3445\n",
      "Epoch 12/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 29.3995 - mae: 3.8850 - val_loss: 41.2705 - val_mae: 3.9380\n",
      "Epoch 13/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 27.2990 - mae: 3.7446 - val_loss: 37.1376 - val_mae: 3.9057\n",
      "Epoch 14/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 23.9468 - mae: 3.5126 - val_loss: 36.3430 - val_mae: 3.6326\n",
      "Epoch 15/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 22.9993 - mae: 3.3577 - val_loss: 31.9864 - val_mae: 3.5650\n",
      "Epoch 16/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 21.1353 - mae: 3.2842 - val_loss: 30.7477 - val_mae: 3.2486\n",
      "Epoch 17/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 20.0809 - mae: 3.1490 - val_loss: 28.5290 - val_mae: 3.2359\n",
      "Epoch 18/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 18.5079 - mae: 3.0122 - val_loss: 26.0848 - val_mae: 3.2187\n",
      "Epoch 19/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 17.6714 - mae: 2.9882 - val_loss: 24.6779 - val_mae: 3.0303\n",
      "Epoch 20/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 16.8349 - mae: 2.8435 - val_loss: 23.3535 - val_mae: 2.8320\n",
      "Epoch 21/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 15.7661 - mae: 2.7342 - val_loss: 22.1711 - val_mae: 2.8119\n",
      "Epoch 22/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 15.0637 - mae: 2.6760 - val_loss: 22.8997 - val_mae: 2.8285\n",
      "Epoch 23/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 14.7631 - mae: 2.6029 - val_loss: 19.4710 - val_mae: 2.7241\n",
      "Epoch 24/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 13.8611 - mae: 2.5662 - val_loss: 19.2349 - val_mae: 2.6984\n",
      "Epoch 25/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 13.4303 - mae: 2.5124 - val_loss: 20.3176 - val_mae: 2.7927\n",
      "Epoch 26/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 13.1886 - mae: 2.4385 - val_loss: 18.3950 - val_mae: 2.6201\n",
      "Epoch 27/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 12.7928 - mae: 2.4035 - val_loss: 16.9430 - val_mae: 2.6045\n",
      "Epoch 28/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 12.4377 - mae: 2.3991 - val_loss: 18.1151 - val_mae: 2.7476\n",
      "Epoch 29/600\n",
      "323/323 [==============================] - 0s 31us/sample - loss: 12.1378 - mae: 2.3993 - val_loss: 18.2095 - val_mae: 2.7051\n",
      "Epoch 30/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 11.7976 - mae: 2.3333 - val_loss: 15.3887 - val_mae: 2.4217\n",
      "Epoch 31/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 11.3459 - mae: 2.2750 - val_loss: 16.4413 - val_mae: 2.4857\n",
      "Epoch 32/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 11.4718 - mae: 2.2882 - val_loss: 17.0751 - val_mae: 2.4879\n",
      "Epoch 33/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 11.1219 - mae: 2.2340 - val_loss: 15.6084 - val_mae: 2.4458\n",
      "Epoch 34/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 10.7045 - mae: 2.2118 - val_loss: 14.5320 - val_mae: 2.3822\n",
      "Epoch 35/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 10.8781 - mae: 2.2508 - val_loss: 17.4443 - val_mae: 2.7244\n",
      "Epoch 36/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 10.7331 - mae: 2.2500 - val_loss: 15.0780 - val_mae: 2.4147\n",
      "Epoch 37/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 10.7206 - mae: 2.1985 - val_loss: 16.6655 - val_mae: 2.5499\n",
      "Epoch 38/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 10.3868 - mae: 2.1515 - val_loss: 14.4861 - val_mae: 2.3651\n",
      "Epoch 39/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 10.1724 - mae: 2.1536 - val_loss: 15.7023 - val_mae: 2.4872\n",
      "Epoch 40/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 10.0385 - mae: 2.1636 - val_loss: 18.3181 - val_mae: 2.7168\n",
      "Epoch 41/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 10.4374 - mae: 2.1514 - val_loss: 15.1480 - val_mae: 2.5817\n",
      "Epoch 42/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 9.7194 - mae: 2.1378 - val_loss: 13.3094 - val_mae: 2.3494\n",
      "Epoch 43/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 10.0457 - mae: 2.1482 - val_loss: 14.0992 - val_mae: 2.3818\n",
      "Epoch 44/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 9.6112 - mae: 2.1012 - val_loss: 15.6881 - val_mae: 2.5976\n",
      "Epoch 45/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 10.0150 - mae: 2.1919 - val_loss: 15.5386 - val_mae: 2.5368\n",
      "Epoch 46/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 9.4130 - mae: 2.0732 - val_loss: 14.4519 - val_mae: 2.4357\n",
      "Epoch 47/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 9.4910 - mae: 2.0869 - val_loss: 15.2783 - val_mae: 2.4303\n",
      "Epoch 48/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 9.4309 - mae: 2.0454 - val_loss: 16.4491 - val_mae: 2.8126\n",
      "Epoch 49/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 9.5932 - mae: 2.1770 - val_loss: 15.2186 - val_mae: 2.5310\n",
      "Epoch 50/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 9.1459 - mae: 2.0965 - val_loss: 15.7184 - val_mae: 2.5946\n",
      "Epoch 51/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 9.1774 - mae: 2.0918 - val_loss: 13.4510 - val_mae: 2.3761\n",
      "Epoch 52/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 9.3354 - mae: 2.0730 - val_loss: 15.2076 - val_mae: 2.4876\n",
      "Epoch 53/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 9.1995 - mae: 2.0538 - val_loss: 14.4698 - val_mae: 2.5135\n",
      "Epoch 54/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 9.2732 - mae: 2.0925 - val_loss: 14.2152 - val_mae: 2.3943\n",
      "Epoch 55/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 9.0839 - mae: 2.0670 - val_loss: 14.9337 - val_mae: 2.4764\n",
      "Epoch 56/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 9.0440 - mae: 2.0500 - val_loss: 16.3217 - val_mae: 2.6426\n",
      "Epoch 57/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 9.0758 - mae: 2.0455 - val_loss: 16.6334 - val_mae: 2.7971\n",
      "Epoch 58/600\n",
      "323/323 [==============================] - 0s 31us/sample - loss: 8.6117 - mae: 2.0150 - val_loss: 12.7264 - val_mae: 2.3666\n",
      "Epoch 59/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.8202 - mae: 2.0419 - val_loss: 14.3943 - val_mae: 2.5128\n",
      "Epoch 60/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.7158 - mae: 2.0404 - val_loss: 14.2695 - val_mae: 2.5348\n",
      "Epoch 61/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.7908 - mae: 2.0331 - val_loss: 15.4315 - val_mae: 2.6613\n",
      "Epoch 62/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.7312 - mae: 2.0133 - val_loss: 13.3186 - val_mae: 2.3615\n",
      "Epoch 63/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.2381 - mae: 2.0085 - val_loss: 14.2174 - val_mae: 2.3963\n",
      "Epoch 64/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.5019 - mae: 2.0151 - val_loss: 13.6869 - val_mae: 2.4019\n",
      "Epoch 65/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.3552 - mae: 2.0167 - val_loss: 12.8949 - val_mae: 2.3155\n",
      "Epoch 66/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.0931 - mae: 1.9343 - val_loss: 14.0282 - val_mae: 2.4709\n",
      "Epoch 67/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 8.1654 - mae: 1.9724 - val_loss: 13.9871 - val_mae: 2.4960\n",
      "Epoch 68/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.2231 - mae: 1.9888 - val_loss: 13.7918 - val_mae: 2.3837\n",
      "Epoch 69/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.2635 - mae: 1.9611 - val_loss: 12.1776 - val_mae: 2.3503\n",
      "Epoch 70/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.9043 - mae: 1.9826 - val_loss: 15.1175 - val_mae: 2.5436\n",
      "Epoch 71/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.1961 - mae: 1.9529 - val_loss: 13.0262 - val_mae: 2.3539\n",
      "Epoch 72/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 8.0466 - mae: 1.9669 - val_loss: 13.3320 - val_mae: 2.3803\n",
      "Epoch 73/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.8137 - mae: 1.9335 - val_loss: 15.4891 - val_mae: 2.5305\n",
      "Epoch 74/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.6746 - mae: 1.9096 - val_loss: 13.9335 - val_mae: 2.4719\n",
      "Epoch 75/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.8244 - mae: 1.9570 - val_loss: 15.3143 - val_mae: 2.8342\n",
      "Epoch 76/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.9107 - mae: 1.9905 - val_loss: 12.6919 - val_mae: 2.3080\n",
      "Epoch 77/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.8178 - mae: 1.9721 - val_loss: 12.7243 - val_mae: 2.3424\n",
      "Epoch 78/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.7699 - mae: 1.9215 - val_loss: 11.6261 - val_mae: 2.2832\n",
      "Epoch 79/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.7360 - mae: 1.9709 - val_loss: 13.3984 - val_mae: 2.4039\n",
      "Epoch 80/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.4894 - mae: 1.9387 - val_loss: 13.5250 - val_mae: 2.3667\n",
      "Epoch 81/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.5955 - mae: 1.8990 - val_loss: 11.4878 - val_mae: 2.3072\n",
      "Epoch 82/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 7.8286 - mae: 1.9749 - val_loss: 14.4188 - val_mae: 2.7323\n",
      "Epoch 83/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.5383 - mae: 1.9333 - val_loss: 12.3387 - val_mae: 2.3420\n",
      "Epoch 84/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.4603 - mae: 1.9036 - val_loss: 13.7104 - val_mae: 2.4612\n",
      "Epoch 85/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.4074 - mae: 1.8795 - val_loss: 12.7160 - val_mae: 2.3957\n",
      "Epoch 86/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.3067 - mae: 1.8858 - val_loss: 14.4290 - val_mae: 2.7070\n",
      "Epoch 87/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.5081 - mae: 1.8998 - val_loss: 11.4847 - val_mae: 2.2583\n",
      "Epoch 88/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.3364 - mae: 1.8735 - val_loss: 13.2095 - val_mae: 2.4368\n",
      "Epoch 89/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 7.2743 - mae: 1.9074 - val_loss: 15.0616 - val_mae: 2.8164\n",
      "Epoch 90/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 7.3634 - mae: 1.9422 - val_loss: 13.6071 - val_mae: 2.5234\n",
      "Epoch 91/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 7.1630 - mae: 1.8892 - val_loss: 13.6909 - val_mae: 2.4744\n",
      "Epoch 92/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 7.2824 - mae: 1.8739 - val_loss: 11.7716 - val_mae: 2.3677\n",
      "Epoch 93/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 7.1643 - mae: 1.8680 - val_loss: 10.8340 - val_mae: 2.2905\n",
      "Epoch 94/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 7.4727 - mae: 1.9548 - val_loss: 12.8441 - val_mae: 2.5737\n",
      "Epoch 95/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 7.0673 - mae: 1.9074 - val_loss: 10.9651 - val_mae: 2.2820\n",
      "Epoch 96/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 7.1271 - mae: 1.8782 - val_loss: 13.1855 - val_mae: 2.4245\n",
      "Epoch 97/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 7.0852 - mae: 1.8715 - val_loss: 15.1814 - val_mae: 2.6218\n",
      "Epoch 98/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 6.8990 - mae: 1.8637 - val_loss: 13.6678 - val_mae: 2.6412\n",
      "Epoch 99/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 7.0115 - mae: 1.8783 - val_loss: 13.8927 - val_mae: 2.4760\n",
      "Epoch 100/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.8046 - mae: 1.8304 - val_loss: 12.6842 - val_mae: 2.3826\n",
      "Epoch 101/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.7766 - mae: 1.8305 - val_loss: 11.0614 - val_mae: 2.2370\n",
      "Epoch 102/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.9140 - mae: 1.8259 - val_loss: 10.4482 - val_mae: 2.2765\n",
      "Epoch 103/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.6570 - mae: 1.8617 - val_loss: 11.4155 - val_mae: 2.3156\n",
      "Epoch 104/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 6.7104 - mae: 1.8089 - val_loss: 13.1571 - val_mae: 2.4992\n",
      "Epoch 105/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 6.7722 - mae: 1.8469 - val_loss: 12.3771 - val_mae: 2.3948\n",
      "Epoch 106/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.6026 - mae: 1.7999 - val_loss: 10.4519 - val_mae: 2.3025\n",
      "Epoch 107/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.6641 - mae: 1.8220 - val_loss: 11.7994 - val_mae: 2.4282\n",
      "Epoch 108/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 6.5179 - mae: 1.7979 - val_loss: 14.1254 - val_mae: 2.7430\n",
      "Epoch 109/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 6.5619 - mae: 1.8207 - val_loss: 13.8242 - val_mae: 2.4960\n",
      "Epoch 110/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 6.3677 - mae: 1.7716 - val_loss: 14.1988 - val_mae: 2.7935\n",
      "Epoch 111/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 6.5612 - mae: 1.8099 - val_loss: 11.7748 - val_mae: 2.2752\n",
      "Epoch 112/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.5725 - mae: 1.7871 - val_loss: 11.6382 - val_mae: 2.3732\n",
      "Epoch 113/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.3654 - mae: 1.8047 - val_loss: 10.5052 - val_mae: 2.3079\n",
      "Epoch 114/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 6.2823 - mae: 1.7805 - val_loss: 12.5614 - val_mae: 2.4173\n",
      "Epoch 115/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.2264 - mae: 1.7635 - val_loss: 14.0055 - val_mae: 2.8018\n",
      "Epoch 116/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.8261 - mae: 1.8398 - val_loss: 13.2359 - val_mae: 2.6357\n",
      "Epoch 117/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 6.1490 - mae: 1.7757 - val_loss: 13.2800 - val_mae: 2.5416\n",
      "Epoch 118/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.1553 - mae: 1.7532 - val_loss: 11.6692 - val_mae: 2.3635\n",
      "Epoch 119/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.0748 - mae: 1.7260 - val_loss: 12.8886 - val_mae: 2.4353\n",
      "Epoch 120/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.1648 - mae: 1.7760 - val_loss: 14.9359 - val_mae: 2.6870\n",
      "Epoch 121/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 6.2194 - mae: 1.7961 - val_loss: 13.8938 - val_mae: 2.7347\n",
      "Epoch 122/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.1337 - mae: 1.7437 - val_loss: 11.7110 - val_mae: 2.4357\n",
      "Epoch 123/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.0138 - mae: 1.7435 - val_loss: 14.7230 - val_mae: 2.8641\n",
      "Epoch 124/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.1747 - mae: 1.7952 - val_loss: 11.5242 - val_mae: 2.3073\n",
      "Epoch 125/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.8845 - mae: 1.6945 - val_loss: 11.5298 - val_mae: 2.3237\n",
      "Epoch 126/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.8322 - mae: 1.6877 - val_loss: 12.9137 - val_mae: 2.4179\n",
      "Epoch 127/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.9936 - mae: 1.6976 - val_loss: 10.8542 - val_mae: 2.2500\n",
      "Epoch 128/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.8393 - mae: 1.7144 - val_loss: 12.6681 - val_mae: 2.4510\n",
      "Epoch 129/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.9014 - mae: 1.7167 - val_loss: 14.8380 - val_mae: 2.6524\n",
      "Epoch 130/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.9334 - mae: 1.7037 - val_loss: 15.9343 - val_mae: 3.0699\n",
      "Epoch 131/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 6.0726 - mae: 1.7718 - val_loss: 9.7704 - val_mae: 2.1606\n",
      "Epoch 132/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.6933 - mae: 1.7155 - val_loss: 10.8031 - val_mae: 2.2789\n",
      "Epoch 133/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.7056 - mae: 1.6921 - val_loss: 11.6664 - val_mae: 2.3287\n",
      "Epoch 134/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.8081 - mae: 1.7430 - val_loss: 12.1004 - val_mae: 2.6458\n",
      "Epoch 135/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.7789 - mae: 1.7495 - val_loss: 10.8745 - val_mae: 2.2469\n",
      "Epoch 136/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.6238 - mae: 1.7073 - val_loss: 10.7117 - val_mae: 2.2458\n",
      "Epoch 137/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.6390 - mae: 1.6780 - val_loss: 11.6231 - val_mae: 2.4436\n",
      "Epoch 138/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.5381 - mae: 1.6652 - val_loss: 13.3984 - val_mae: 2.6396\n",
      "Epoch 139/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.5012 - mae: 1.6523 - val_loss: 9.5212 - val_mae: 2.2118\n",
      "Epoch 140/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.5931 - mae: 1.7141 - val_loss: 12.4194 - val_mae: 2.5571\n",
      "Epoch 141/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.4318 - mae: 1.6644 - val_loss: 12.9916 - val_mae: 2.4125\n",
      "Epoch 142/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.4184 - mae: 1.6522 - val_loss: 11.1364 - val_mae: 2.3592\n",
      "Epoch 143/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.4719 - mae: 1.6475 - val_loss: 11.8457 - val_mae: 2.4381\n",
      "Epoch 144/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.3655 - mae: 1.6316 - val_loss: 11.8556 - val_mae: 2.5644\n",
      "Epoch 145/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.3484 - mae: 1.6307 - val_loss: 11.9479 - val_mae: 2.3855\n",
      "Epoch 146/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.2856 - mae: 1.6123 - val_loss: 11.2487 - val_mae: 2.3595\n",
      "Epoch 147/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.2849 - mae: 1.6265 - val_loss: 11.5005 - val_mae: 2.6088\n",
      "Epoch 148/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.4561 - mae: 1.6777 - val_loss: 11.0346 - val_mae: 2.3759\n",
      "Epoch 149/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.4027 - mae: 1.6807 - val_loss: 13.7297 - val_mae: 2.8405\n",
      "Epoch 150/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.1916 - mae: 1.6306 - val_loss: 12.6336 - val_mae: 2.6023\n",
      "Epoch 151/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.3084 - mae: 1.6359 - val_loss: 11.4556 - val_mae: 2.4430\n",
      "Epoch 152/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.2245 - mae: 1.6436 - val_loss: 11.0696 - val_mae: 2.3543\n",
      "Epoch 153/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.1508 - mae: 1.6289 - val_loss: 11.4598 - val_mae: 2.3594\n",
      "Epoch 154/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.1843 - mae: 1.6229 - val_loss: 13.2404 - val_mae: 2.6885\n",
      "Epoch 155/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.2557 - mae: 1.6356 - val_loss: 11.8022 - val_mae: 2.4324\n",
      "Epoch 156/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.2018 - mae: 1.6219 - val_loss: 11.3359 - val_mae: 2.4276\n",
      "Epoch 157/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 5.0093 - mae: 1.6282 - val_loss: 12.0921 - val_mae: 2.4798\n",
      "Epoch 158/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.9158 - mae: 1.5801 - val_loss: 14.1215 - val_mae: 2.8022\n",
      "Epoch 159/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.9975 - mae: 1.5922 - val_loss: 11.9593 - val_mae: 2.4386\n",
      "Epoch 160/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.1765 - mae: 1.6373 - val_loss: 10.3894 - val_mae: 2.3517\n",
      "Epoch 161/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.8668 - mae: 1.5665 - val_loss: 11.0086 - val_mae: 2.3643\n",
      "Epoch 162/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.7379 - mae: 1.5557 - val_loss: 10.6590 - val_mae: 2.4408\n",
      "Epoch 163/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.0167 - mae: 1.6306 - val_loss: 14.3886 - val_mae: 2.9349\n",
      "Epoch 164/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.8293 - mae: 1.5933 - val_loss: 12.2191 - val_mae: 2.4687\n",
      "Epoch 165/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 5.0602 - mae: 1.6175 - val_loss: 11.4354 - val_mae: 2.5107\n",
      "Epoch 166/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.7081 - mae: 1.5446 - val_loss: 12.3403 - val_mae: 2.6182\n",
      "Epoch 167/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.7056 - mae: 1.5716 - val_loss: 15.3610 - val_mae: 3.0249\n",
      "Epoch 168/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.8845 - mae: 1.6132 - val_loss: 10.7978 - val_mae: 2.3825\n",
      "Epoch 169/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.6705 - mae: 1.5710 - val_loss: 12.6646 - val_mae: 2.4844\n",
      "Epoch 170/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.7067 - mae: 1.5551 - val_loss: 14.4795 - val_mae: 2.9349\n",
      "Epoch 171/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.8107 - mae: 1.5609 - val_loss: 9.7905 - val_mae: 2.2356\n",
      "Epoch 172/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.4948 - mae: 1.5285 - val_loss: 9.5224 - val_mae: 2.1782\n",
      "Epoch 173/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.6551 - mae: 1.5635 - val_loss: 11.5677 - val_mae: 2.4677\n",
      "Epoch 174/600\n",
      "323/323 [==============================] - 0s 71us/sample - loss: 4.4539 - mae: 1.5200 - val_loss: 12.7928 - val_mae: 2.6120\n",
      "Epoch 175/600\n",
      "323/323 [==============================] - 0s 43us/sample - loss: 4.7461 - mae: 1.5588 - val_loss: 13.3710 - val_mae: 2.5651\n",
      "Epoch 176/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 4.4855 - mae: 1.4813 - val_loss: 10.8024 - val_mae: 2.4488\n",
      "Epoch 177/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.4846 - mae: 1.5050 - val_loss: 13.1742 - val_mae: 2.7951\n",
      "Epoch 178/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.7573 - mae: 1.5660 - val_loss: 10.3170 - val_mae: 2.2932\n",
      "Epoch 179/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.3987 - mae: 1.5012 - val_loss: 10.7560 - val_mae: 2.2813\n",
      "Epoch 180/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.5282 - mae: 1.5191 - val_loss: 10.6602 - val_mae: 2.3603\n",
      "Epoch 181/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.4822 - mae: 1.5171 - val_loss: 12.7834 - val_mae: 2.7717\n",
      "Epoch 182/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.3437 - mae: 1.5085 - val_loss: 10.7333 - val_mae: 2.3565\n",
      "Epoch 183/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.3857 - mae: 1.5193 - val_loss: 10.6690 - val_mae: 2.3664\n",
      "Epoch 184/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.2684 - mae: 1.4979 - val_loss: 12.2363 - val_mae: 2.5286\n",
      "Epoch 185/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.3086 - mae: 1.4793 - val_loss: 11.6473 - val_mae: 2.4495\n",
      "Epoch 186/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.2556 - mae: 1.5108 - val_loss: 10.4756 - val_mae: 2.4040\n",
      "Epoch 187/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.2634 - mae: 1.5161 - val_loss: 9.8444 - val_mae: 2.2888\n",
      "Epoch 188/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.2783 - mae: 1.4921 - val_loss: 10.0915 - val_mae: 2.2863\n",
      "Epoch 189/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.2172 - mae: 1.4924 - val_loss: 12.6985 - val_mae: 2.7441\n",
      "Epoch 190/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.3507 - mae: 1.5165 - val_loss: 10.9614 - val_mae: 2.4190\n",
      "Epoch 191/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.9877 - mae: 1.4634 - val_loss: 11.5421 - val_mae: 2.5076\n",
      "Epoch 192/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.2903 - mae: 1.4887 - val_loss: 11.6461 - val_mae: 2.4337\n",
      "Epoch 193/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.1601 - mae: 1.4602 - val_loss: 12.4295 - val_mae: 2.6712\n",
      "Epoch 194/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.1368 - mae: 1.4703 - val_loss: 14.0278 - val_mae: 2.8945\n",
      "Epoch 195/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.2895 - mae: 1.5043 - val_loss: 16.6035 - val_mae: 3.1777\n",
      "Epoch 196/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 4.5565 - mae: 1.5651 - val_loss: 12.1521 - val_mae: 2.6248\n",
      "Epoch 197/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.0770 - mae: 1.4423 - val_loss: 11.5090 - val_mae: 2.3777\n",
      "Epoch 198/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.1253 - mae: 1.4396 - val_loss: 10.7540 - val_mae: 2.3208\n",
      "Epoch 199/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.9541 - mae: 1.4167 - val_loss: 11.5994 - val_mae: 2.5161\n",
      "Epoch 200/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.8665 - mae: 1.4190 - val_loss: 12.3912 - val_mae: 2.6127\n",
      "Epoch 201/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.0998 - mae: 1.4525 - val_loss: 12.8253 - val_mae: 2.6119\n",
      "Epoch 202/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.0549 - mae: 1.4326 - val_loss: 11.5593 - val_mae: 2.5045\n",
      "Epoch 203/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.9980 - mae: 1.4186 - val_loss: 11.7675 - val_mae: 2.4659\n",
      "Epoch 204/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 4.1566 - mae: 1.4746 - val_loss: 12.1231 - val_mae: 2.4804\n",
      "Epoch 205/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.8531 - mae: 1.4315 - val_loss: 10.9513 - val_mae: 2.4838\n",
      "Epoch 206/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.8136 - mae: 1.3996 - val_loss: 12.4330 - val_mae: 2.6784\n",
      "Epoch 207/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.8998 - mae: 1.4317 - val_loss: 11.4289 - val_mae: 2.4240\n",
      "Epoch 208/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.8671 - mae: 1.4289 - val_loss: 14.6351 - val_mae: 2.9961\n",
      "Epoch 209/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.9593 - mae: 1.4144 - val_loss: 9.7016 - val_mae: 2.1736\n",
      "Epoch 210/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.8489 - mae: 1.4324 - val_loss: 12.0878 - val_mae: 2.6758\n",
      "Epoch 211/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.7714 - mae: 1.3804 - val_loss: 12.6495 - val_mae: 2.7782\n",
      "Epoch 212/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.8499 - mae: 1.4223 - val_loss: 14.7728 - val_mae: 2.8931\n",
      "Epoch 213/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.9727 - mae: 1.4428 - val_loss: 10.1303 - val_mae: 2.2477\n",
      "Epoch 214/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.9008 - mae: 1.4357 - val_loss: 10.8060 - val_mae: 2.4037\n",
      "Epoch 215/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.8525 - mae: 1.4435 - val_loss: 10.9864 - val_mae: 2.3558\n",
      "Epoch 216/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.7958 - mae: 1.4005 - val_loss: 11.7772 - val_mae: 2.4316\n",
      "Epoch 217/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.8852 - mae: 1.4247 - val_loss: 12.4836 - val_mae: 2.6728\n",
      "Epoch 218/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.7580 - mae: 1.4120 - val_loss: 10.5167 - val_mae: 2.3621\n",
      "Epoch 219/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.9006 - mae: 1.4479 - val_loss: 13.0870 - val_mae: 2.5864\n",
      "Epoch 220/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.8395 - mae: 1.4286 - val_loss: 11.8116 - val_mae: 2.6551\n",
      "Epoch 221/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.6690 - mae: 1.3885 - val_loss: 14.2398 - val_mae: 2.8936\n",
      "Epoch 222/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.6145 - mae: 1.3595 - val_loss: 11.1769 - val_mae: 2.5062\n",
      "Epoch 223/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.8053 - mae: 1.4366 - val_loss: 11.0174 - val_mae: 2.3687\n",
      "Epoch 224/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.5213 - mae: 1.3607 - val_loss: 15.1334 - val_mae: 2.9767\n",
      "Epoch 225/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.8197 - mae: 1.3999 - val_loss: 12.1732 - val_mae: 2.6726\n",
      "Epoch 226/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.5002 - mae: 1.3454 - val_loss: 10.6863 - val_mae: 2.3886\n",
      "Epoch 227/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.5034 - mae: 1.3495 - val_loss: 11.8325 - val_mae: 2.6179\n",
      "Epoch 228/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.4867 - mae: 1.3667 - val_loss: 11.1200 - val_mae: 2.4741\n",
      "Epoch 229/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.5300 - mae: 1.3701 - val_loss: 12.8811 - val_mae: 2.7612\n",
      "Epoch 230/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.3805 - mae: 1.3435 - val_loss: 13.0858 - val_mae: 2.6727\n",
      "Epoch 231/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.6081 - mae: 1.3433 - val_loss: 9.7561 - val_mae: 2.2915\n",
      "Epoch 232/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.6741 - mae: 1.4307 - val_loss: 9.8363 - val_mae: 2.3036\n",
      "Epoch 233/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.7607 - mae: 1.4204 - val_loss: 10.3994 - val_mae: 2.2880\n",
      "Epoch 234/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.3880 - mae: 1.3486 - val_loss: 11.5951 - val_mae: 2.5978\n",
      "Epoch 235/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.4463 - mae: 1.3638 - val_loss: 12.9706 - val_mae: 2.7835\n",
      "Epoch 236/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.3796 - mae: 1.3307 - val_loss: 11.9219 - val_mae: 2.6422\n",
      "Epoch 237/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.3159 - mae: 1.3282 - val_loss: 9.4598 - val_mae: 2.2415\n",
      "Epoch 238/600\n",
      "323/323 [==============================] - 0s 65us/sample - loss: 3.4422 - mae: 1.3646 - val_loss: 10.2088 - val_mae: 2.3852\n",
      "Epoch 239/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.4083 - mae: 1.3434 - val_loss: 10.1560 - val_mae: 2.2774\n",
      "Epoch 240/600\n",
      "323/323 [==============================] - 0s 43us/sample - loss: 3.4601 - mae: 1.3637 - val_loss: 10.7936 - val_mae: 2.4839\n",
      "Epoch 241/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.4233 - mae: 1.3859 - val_loss: 15.3447 - val_mae: 2.8872\n",
      "Epoch 242/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.6557 - mae: 1.3762 - val_loss: 12.1761 - val_mae: 2.6022\n",
      "Epoch 243/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.2459 - mae: 1.3257 - val_loss: 11.9519 - val_mae: 2.5522\n",
      "Epoch 244/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.4804 - mae: 1.3468 - val_loss: 11.1964 - val_mae: 2.5483\n",
      "Epoch 245/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.3986 - mae: 1.3523 - val_loss: 11.0300 - val_mae: 2.5490\n",
      "Epoch 246/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.2956 - mae: 1.3168 - val_loss: 10.3175 - val_mae: 2.3572\n",
      "Epoch 247/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.2823 - mae: 1.3390 - val_loss: 10.1476 - val_mae: 2.3226\n",
      "Epoch 248/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.0897 - mae: 1.2843 - val_loss: 12.8217 - val_mae: 2.5373\n",
      "Epoch 249/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.4941 - mae: 1.3792 - val_loss: 9.9477 - val_mae: 2.2304\n",
      "Epoch 250/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.3228 - mae: 1.3118 - val_loss: 11.4736 - val_mae: 2.4704\n",
      "Epoch 251/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.0801 - mae: 1.2591 - val_loss: 12.8038 - val_mae: 2.6310\n",
      "Epoch 252/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.4309 - mae: 1.3375 - val_loss: 11.0094 - val_mae: 2.4496\n",
      "Epoch 253/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.9370 - mae: 1.2555 - val_loss: 14.0874 - val_mae: 2.9362\n",
      "Epoch 254/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.4077 - mae: 1.3386 - val_loss: 12.1288 - val_mae: 2.6324\n",
      "Epoch 255/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.9324 - mae: 1.2563 - val_loss: 9.6903 - val_mae: 2.2750\n",
      "Epoch 256/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.2158 - mae: 1.3302 - val_loss: 9.9945 - val_mae: 2.3323\n",
      "Epoch 257/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.0562 - mae: 1.2674 - val_loss: 11.0487 - val_mae: 2.4332\n",
      "Epoch 258/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.1781 - mae: 1.2823 - val_loss: 11.6604 - val_mae: 2.4513\n",
      "Epoch 259/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.1217 - mae: 1.2938 - val_loss: 13.5172 - val_mae: 2.7748\n",
      "Epoch 260/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.1127 - mae: 1.2806 - val_loss: 10.0625 - val_mae: 2.2944\n",
      "Epoch 261/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.0874 - mae: 1.2949 - val_loss: 10.3536 - val_mae: 2.3800\n",
      "Epoch 262/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.0453 - mae: 1.2954 - val_loss: 11.0896 - val_mae: 2.4972\n",
      "Epoch 263/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.1179 - mae: 1.2864 - val_loss: 15.2652 - val_mae: 3.1144\n",
      "Epoch 264/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.9937 - mae: 1.2501 - val_loss: 10.7884 - val_mae: 2.4750\n",
      "Epoch 265/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 2.9974 - mae: 1.2476 - val_loss: 16.3388 - val_mae: 3.0329\n",
      "Epoch 266/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.3749 - mae: 1.3073 - val_loss: 10.9384 - val_mae: 2.4775\n",
      "Epoch 267/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.9148 - mae: 1.2432 - val_loss: 12.9896 - val_mae: 2.8211\n",
      "Epoch 268/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.9466 - mae: 1.2660 - val_loss: 11.6209 - val_mae: 2.6053\n",
      "Epoch 269/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.9839 - mae: 1.2804 - val_loss: 9.9070 - val_mae: 2.3249\n",
      "Epoch 270/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.1066 - mae: 1.2741 - val_loss: 9.9230 - val_mae: 2.2540\n",
      "Epoch 271/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.7838 - mae: 1.2290 - val_loss: 11.7131 - val_mae: 2.6575\n",
      "Epoch 272/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.8605 - mae: 1.2417 - val_loss: 10.0910 - val_mae: 2.2780\n",
      "Epoch 273/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.0354 - mae: 1.2868 - val_loss: 14.5962 - val_mae: 2.9180\n",
      "Epoch 274/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.0769 - mae: 1.2495 - val_loss: 11.2833 - val_mae: 2.5106\n",
      "Epoch 275/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.9024 - mae: 1.2454 - val_loss: 11.4567 - val_mae: 2.5765\n",
      "Epoch 276/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.9270 - mae: 1.2595 - val_loss: 12.9922 - val_mae: 2.8318\n",
      "Epoch 277/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.8531 - mae: 1.2491 - val_loss: 14.2309 - val_mae: 2.9255\n",
      "Epoch 278/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.0329 - mae: 1.3036 - val_loss: 12.2045 - val_mae: 2.5222\n",
      "Epoch 279/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.8326 - mae: 1.2398 - val_loss: 9.7396 - val_mae: 2.2538\n",
      "Epoch 280/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.8521 - mae: 1.2407 - val_loss: 12.0133 - val_mae: 2.6015\n",
      "Epoch 281/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.8674 - mae: 1.2259 - val_loss: 11.3253 - val_mae: 2.5828\n",
      "Epoch 282/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.8316 - mae: 1.2146 - val_loss: 12.0501 - val_mae: 2.7031\n",
      "Epoch 283/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 3.0894 - mae: 1.2795 - val_loss: 13.1198 - val_mae: 2.8461\n",
      "Epoch 284/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.7954 - mae: 1.2469 - val_loss: 11.6827 - val_mae: 2.5890\n",
      "Epoch 285/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.7644 - mae: 1.2286 - val_loss: 12.2172 - val_mae: 2.6399\n",
      "Epoch 286/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 3.0439 - mae: 1.2839 - val_loss: 14.2703 - val_mae: 2.8079\n",
      "Epoch 287/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.7404 - mae: 1.2071 - val_loss: 11.3687 - val_mae: 2.4772\n",
      "Epoch 288/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.7403 - mae: 1.2041 - val_loss: 13.2501 - val_mae: 2.7833\n",
      "Epoch 289/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.7026 - mae: 1.2034 - val_loss: 10.7447 - val_mae: 2.4050\n",
      "Epoch 290/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.6425 - mae: 1.1837 - val_loss: 13.0395 - val_mae: 2.8093\n",
      "Epoch 291/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.7393 - mae: 1.2202 - val_loss: 14.7430 - val_mae: 2.8584\n",
      "Epoch 292/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.8309 - mae: 1.2410 - val_loss: 15.0853 - val_mae: 2.9520\n",
      "Epoch 293/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.8322 - mae: 1.2151 - val_loss: 12.2723 - val_mae: 2.5221\n",
      "Epoch 294/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.6576 - mae: 1.2264 - val_loss: 10.9055 - val_mae: 2.4473\n",
      "Epoch 295/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.7537 - mae: 1.2235 - val_loss: 10.0304 - val_mae: 2.3498\n",
      "Epoch 296/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.5622 - mae: 1.1897 - val_loss: 15.8008 - val_mae: 3.0109\n",
      "Epoch 297/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.7074 - mae: 1.2287 - val_loss: 13.9857 - val_mae: 2.8108\n",
      "Epoch 298/600\n",
      "323/323 [==============================] - 0s 68us/sample - loss: 2.7659 - mae: 1.2158 - val_loss: 12.0646 - val_mae: 2.6912\n",
      "Epoch 299/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.5462 - mae: 1.1836 - val_loss: 16.9449 - val_mae: 3.3382\n",
      "Epoch 300/600\n",
      "323/323 [==============================] - 0s 43us/sample - loss: 3.2157 - mae: 1.2930 - val_loss: 9.6520 - val_mae: 2.2956\n",
      "Epoch 301/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.6149 - mae: 1.1777 - val_loss: 15.4078 - val_mae: 3.1053\n",
      "Epoch 302/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.6463 - mae: 1.1714 - val_loss: 10.1935 - val_mae: 2.3997\n",
      "Epoch 303/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.4330 - mae: 1.1529 - val_loss: 10.7399 - val_mae: 2.5282\n",
      "Epoch 304/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.7167 - mae: 1.2073 - val_loss: 12.3163 - val_mae: 2.7283\n",
      "Epoch 305/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.7051 - mae: 1.2029 - val_loss: 10.7741 - val_mae: 2.4255\n",
      "Epoch 306/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.4355 - mae: 1.1402 - val_loss: 11.3725 - val_mae: 2.4454\n",
      "Epoch 307/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.6577 - mae: 1.1681 - val_loss: 10.5959 - val_mae: 2.3526\n",
      "Epoch 308/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.6752 - mae: 1.2221 - val_loss: 10.6108 - val_mae: 2.4685\n",
      "Epoch 309/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.4525 - mae: 1.1397 - val_loss: 13.2931 - val_mae: 2.8004\n",
      "Epoch 310/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.6804 - mae: 1.1959 - val_loss: 14.2448 - val_mae: 2.9261\n",
      "Epoch 311/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.5739 - mae: 1.1988 - val_loss: 12.2054 - val_mae: 2.5617\n",
      "Epoch 312/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.4518 - mae: 1.1425 - val_loss: 11.4279 - val_mae: 2.6416\n",
      "Epoch 313/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.4686 - mae: 1.1587 - val_loss: 14.9798 - val_mae: 2.9268\n",
      "Epoch 314/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.7974 - mae: 1.1955 - val_loss: 10.3986 - val_mae: 2.3577\n",
      "Epoch 315/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.5094 - mae: 1.1607 - val_loss: 10.2256 - val_mae: 2.3322\n",
      "Epoch 316/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.4713 - mae: 1.1664 - val_loss: 10.4954 - val_mae: 2.3702\n",
      "Epoch 317/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.4115 - mae: 1.1442 - val_loss: 11.3759 - val_mae: 2.5457\n",
      "Epoch 318/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.4309 - mae: 1.1536 - val_loss: 11.7334 - val_mae: 2.4354\n",
      "Epoch 319/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.4300 - mae: 1.1662 - val_loss: 10.9092 - val_mae: 2.4991\n",
      "Epoch 320/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.3236 - mae: 1.1192 - val_loss: 12.3742 - val_mae: 2.6561\n",
      "Epoch 321/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.3580 - mae: 1.1345 - val_loss: 13.0383 - val_mae: 2.6509\n",
      "Epoch 322/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.5320 - mae: 1.1425 - val_loss: 14.7298 - val_mae: 3.1093\n",
      "Epoch 323/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.4673 - mae: 1.1626 - val_loss: 11.2993 - val_mae: 2.6167\n",
      "Epoch 324/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.5572 - mae: 1.1863 - val_loss: 10.7196 - val_mae: 2.4030\n",
      "Epoch 325/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.2667 - mae: 1.1086 - val_loss: 11.8218 - val_mae: 2.4300\n",
      "Epoch 326/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.4001 - mae: 1.1428 - val_loss: 13.7992 - val_mae: 2.8471\n",
      "Epoch 327/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.3794 - mae: 1.1437 - val_loss: 11.9472 - val_mae: 2.5959\n",
      "Epoch 328/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.6110 - mae: 1.1628 - val_loss: 12.7565 - val_mae: 2.7288\n",
      "Epoch 329/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.2816 - mae: 1.0897 - val_loss: 12.5151 - val_mae: 2.7865\n",
      "Epoch 330/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.4498 - mae: 1.1650 - val_loss: 10.8125 - val_mae: 2.4433\n",
      "Epoch 331/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.2475 - mae: 1.0868 - val_loss: 10.9537 - val_mae: 2.5322\n",
      "Epoch 332/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.1414 - mae: 1.0783 - val_loss: 12.7225 - val_mae: 2.6688\n",
      "Epoch 333/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.4481 - mae: 1.1177 - val_loss: 11.2395 - val_mae: 2.5313\n",
      "Epoch 334/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.1283 - mae: 1.0652 - val_loss: 11.2922 - val_mae: 2.5012\n",
      "Epoch 335/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.2230 - mae: 1.0803 - val_loss: 13.1553 - val_mae: 2.8409\n",
      "Epoch 336/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.5325 - mae: 1.1784 - val_loss: 11.8808 - val_mae: 2.5216\n",
      "Epoch 337/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.1999 - mae: 1.0892 - val_loss: 12.1566 - val_mae: 2.6347\n",
      "Epoch 338/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.4326 - mae: 1.1728 - val_loss: 13.7897 - val_mae: 2.7224\n",
      "Epoch 339/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.2267 - mae: 1.0906 - val_loss: 10.6035 - val_mae: 2.3358\n",
      "Epoch 340/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.4413 - mae: 1.1516 - val_loss: 9.9463 - val_mae: 2.2869\n",
      "Epoch 341/600\n",
      "323/323 [==============================] - 0s 65us/sample - loss: 2.1445 - mae: 1.0415 - val_loss: 13.9899 - val_mae: 2.8808\n",
      "Epoch 342/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 2.4598 - mae: 1.1837 - val_loss: 10.4655 - val_mae: 2.4484\n",
      "Epoch 343/600\n",
      "323/323 [==============================] - 0s 43us/sample - loss: 2.0796 - mae: 1.0524 - val_loss: 9.7588 - val_mae: 2.3694\n",
      "Epoch 344/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.3498 - mae: 1.1293 - val_loss: 11.4300 - val_mae: 2.5351\n",
      "Epoch 345/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.1706 - mae: 1.0867 - val_loss: 12.5654 - val_mae: 2.6585\n",
      "Epoch 346/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.2118 - mae: 1.0575 - val_loss: 10.5713 - val_mae: 2.5316\n",
      "Epoch 347/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.2637 - mae: 1.1110 - val_loss: 11.1920 - val_mae: 2.4530\n",
      "Epoch 348/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0656 - mae: 1.0453 - val_loss: 13.1901 - val_mae: 2.7118\n",
      "Epoch 349/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.1876 - mae: 1.0845 - val_loss: 10.5575 - val_mae: 2.3878\n",
      "Epoch 350/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.3264 - mae: 1.1147 - val_loss: 10.6023 - val_mae: 2.4482\n",
      "Epoch 351/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.1075 - mae: 1.0762 - val_loss: 11.3486 - val_mae: 2.6032\n",
      "Epoch 352/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.1350 - mae: 1.0799 - val_loss: 11.0298 - val_mae: 2.4588\n",
      "Epoch 353/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.2261 - mae: 1.0811 - val_loss: 10.2442 - val_mae: 2.4186\n",
      "Epoch 354/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.4440 - mae: 1.1593 - val_loss: 11.3396 - val_mae: 2.5293\n",
      "Epoch 355/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.0155 - mae: 1.0526 - val_loss: 11.9469 - val_mae: 2.7162\n",
      "Epoch 356/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.9392 - mae: 1.0349 - val_loss: 10.3968 - val_mae: 2.5502\n",
      "Epoch 357/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.2405 - mae: 1.1499 - val_loss: 10.8048 - val_mae: 2.4506\n",
      "Epoch 358/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.2426 - mae: 1.0830 - val_loss: 10.3608 - val_mae: 2.3912\n",
      "Epoch 359/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0487 - mae: 1.0527 - val_loss: 10.4383 - val_mae: 2.3163\n",
      "Epoch 360/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0845 - mae: 1.0696 - val_loss: 11.3504 - val_mae: 2.6007\n",
      "Epoch 361/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.2526 - mae: 1.1009 - val_loss: 9.7630 - val_mae: 2.3290\n",
      "Epoch 362/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0546 - mae: 1.0569 - val_loss: 10.9282 - val_mae: 2.3468\n",
      "Epoch 363/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0121 - mae: 1.0255 - val_loss: 12.7655 - val_mae: 2.8430\n",
      "Epoch 364/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.1493 - mae: 1.0591 - val_loss: 11.2124 - val_mae: 2.4408\n",
      "Epoch 365/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0870 - mae: 1.0768 - val_loss: 9.8649 - val_mae: 2.3395\n",
      "Epoch 366/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0627 - mae: 1.0510 - val_loss: 11.1146 - val_mae: 2.4652\n",
      "Epoch 367/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.9679 - mae: 1.0246 - val_loss: 10.8030 - val_mae: 2.3880\n",
      "Epoch 368/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 2.0509 - mae: 1.0433 - val_loss: 11.5004 - val_mae: 2.4813\n",
      "Epoch 369/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.9684 - mae: 1.0294 - val_loss: 12.6625 - val_mae: 2.7795\n",
      "Epoch 370/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.2844 - mae: 1.1343 - val_loss: 10.6755 - val_mae: 2.4784\n",
      "Epoch 371/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.9319 - mae: 1.0102 - val_loss: 10.7797 - val_mae: 2.4307\n",
      "Epoch 372/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.9492 - mae: 1.0078 - val_loss: 11.2252 - val_mae: 2.5024\n",
      "Epoch 373/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.9979 - mae: 1.0332 - val_loss: 11.2076 - val_mae: 2.5870\n",
      "Epoch 374/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0524 - mae: 1.0458 - val_loss: 11.6290 - val_mae: 2.4381\n",
      "Epoch 375/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0314 - mae: 1.0631 - val_loss: 10.7169 - val_mae: 2.4644\n",
      "Epoch 376/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.9289 - mae: 1.0154 - val_loss: 11.7493 - val_mae: 2.6319\n",
      "Epoch 377/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.0775 - mae: 1.0706 - val_loss: 10.4810 - val_mae: 2.4040\n",
      "Epoch 378/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.0032 - mae: 1.0154 - val_loss: 13.5973 - val_mae: 2.7506\n",
      "Epoch 379/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.0601 - mae: 1.0565 - val_loss: 10.1042 - val_mae: 2.3515\n",
      "Epoch 380/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.9472 - mae: 1.0319 - val_loss: 11.5611 - val_mae: 2.6502\n",
      "Epoch 381/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.9214 - mae: 1.0101 - val_loss: 12.1496 - val_mae: 2.5877\n",
      "Epoch 382/600\n",
      "323/323 [==============================] - 0s 68us/sample - loss: 1.9433 - mae: 1.0622 - val_loss: 12.3595 - val_mae: 2.5876\n",
      "Epoch 383/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0735 - mae: 1.0374 - val_loss: 11.0213 - val_mae: 2.4599\n",
      "Epoch 384/600\n",
      "323/323 [==============================] - 0s 43us/sample - loss: 2.1143 - mae: 1.0419 - val_loss: 10.9859 - val_mae: 2.5277\n",
      "Epoch 385/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8473 - mae: 0.9880 - val_loss: 11.1681 - val_mae: 2.6072\n",
      "Epoch 386/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.9739 - mae: 1.0625 - val_loss: 11.7405 - val_mae: 2.6958\n",
      "Epoch 387/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.8996 - mae: 1.0066 - val_loss: 11.7251 - val_mae: 2.6351\n",
      "Epoch 388/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.0048 - mae: 1.0756 - val_loss: 14.3038 - val_mae: 2.8908\n",
      "Epoch 389/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 1.8739 - mae: 0.9943 - val_loss: 12.9418 - val_mae: 2.8368\n",
      "Epoch 390/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0732 - mae: 1.0342 - val_loss: 11.0727 - val_mae: 2.5511\n",
      "Epoch 391/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8760 - mae: 0.9998 - val_loss: 11.1062 - val_mae: 2.4986\n",
      "Epoch 392/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.9242 - mae: 1.0144 - val_loss: 11.2680 - val_mae: 2.5057\n",
      "Epoch 393/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8318 - mae: 0.9963 - val_loss: 11.3150 - val_mae: 2.5037\n",
      "Epoch 394/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.8134 - mae: 1.0017 - val_loss: 12.5492 - val_mae: 2.5888\n",
      "Epoch 395/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.0603 - mae: 1.0711 - val_loss: 13.5106 - val_mae: 2.7148\n",
      "Epoch 396/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.1425 - mae: 1.0520 - val_loss: 11.0139 - val_mae: 2.4045\n",
      "Epoch 397/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8035 - mae: 0.9822 - val_loss: 11.4507 - val_mae: 2.7136\n",
      "Epoch 398/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8457 - mae: 0.9932 - val_loss: 10.5280 - val_mae: 2.4416\n",
      "Epoch 399/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8383 - mae: 0.9960 - val_loss: 10.6117 - val_mae: 2.6220\n",
      "Epoch 400/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8520 - mae: 1.0156 - val_loss: 11.5739 - val_mae: 2.5584\n",
      "Epoch 401/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.8261 - mae: 1.0158 - val_loss: 10.8212 - val_mae: 2.4420\n",
      "Epoch 402/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0875 - mae: 1.0964 - val_loss: 11.4028 - val_mae: 2.5478\n",
      "Epoch 403/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.7574 - mae: 0.9569 - val_loss: 11.1058 - val_mae: 2.5784\n",
      "Epoch 404/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.7375 - mae: 0.9725 - val_loss: 12.4051 - val_mae: 2.7720\n",
      "Epoch 405/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.9462 - mae: 1.0293 - val_loss: 14.0846 - val_mae: 2.8740\n",
      "Epoch 406/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.9769 - mae: 1.0300 - val_loss: 12.3331 - val_mae: 2.6263\n",
      "Epoch 407/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8469 - mae: 0.9475 - val_loss: 14.7785 - val_mae: 2.9468\n",
      "Epoch 408/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0871 - mae: 1.0532 - val_loss: 11.6942 - val_mae: 2.6017\n",
      "Epoch 409/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.6038 - mae: 0.9340 - val_loss: 9.8199 - val_mae: 2.3440\n",
      "Epoch 410/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.7405 - mae: 0.9669 - val_loss: 14.5275 - val_mae: 3.0356\n",
      "Epoch 411/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.7946 - mae: 0.9916 - val_loss: 10.9897 - val_mae: 2.5161\n",
      "Epoch 412/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.7191 - mae: 0.9628 - val_loss: 10.9633 - val_mae: 2.6047\n",
      "Epoch 413/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.1284 - mae: 1.0411 - val_loss: 11.7377 - val_mae: 2.6462\n",
      "Epoch 414/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.8106 - mae: 0.9731 - val_loss: 11.0758 - val_mae: 2.5552\n",
      "Epoch 415/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.6838 - mae: 0.9404 - val_loss: 15.3624 - val_mae: 2.9871\n",
      "Epoch 416/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0935 - mae: 1.0345 - val_loss: 13.7170 - val_mae: 2.7962\n",
      "Epoch 417/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.6586 - mae: 0.9550 - val_loss: 10.7494 - val_mae: 2.5055\n",
      "Epoch 418/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5287 - mae: 0.8934 - val_loss: 10.9480 - val_mae: 2.4912\n",
      "Epoch 419/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.7700 - mae: 0.9780 - val_loss: 14.8653 - val_mae: 3.1190\n",
      "Epoch 420/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8331 - mae: 0.9937 - val_loss: 11.2407 - val_mae: 2.4691\n",
      "Epoch 421/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.6382 - mae: 0.9344 - val_loss: 13.7590 - val_mae: 2.9094\n",
      "Epoch 422/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.9680 - mae: 1.0397 - val_loss: 11.7256 - val_mae: 2.5608\n",
      "Epoch 423/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.7894 - mae: 0.9857 - val_loss: 11.6519 - val_mae: 2.5042\n",
      "Epoch 424/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.6783 - mae: 0.9576 - val_loss: 10.4213 - val_mae: 2.4065\n",
      "Epoch 425/600\n",
      "323/323 [==============================] - 0s 46us/sample - loss: 1.7166 - mae: 0.9597 - val_loss: 13.7072 - val_mae: 2.9368\n",
      "Epoch 426/600\n",
      "323/323 [==============================] - 0s 56us/sample - loss: 1.8628 - mae: 0.9818 - val_loss: 14.0016 - val_mae: 2.8851\n",
      "Epoch 427/600\n",
      "323/323 [==============================] - 0s 43us/sample - loss: 1.9121 - mae: 1.0185 - val_loss: 10.6570 - val_mae: 2.5288\n",
      "Epoch 428/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5539 - mae: 0.9260 - val_loss: 17.4500 - val_mae: 3.4349\n",
      "Epoch 429/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 2.0374 - mae: 1.0471 - val_loss: 10.5439 - val_mae: 2.4636\n",
      "Epoch 430/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5983 - mae: 0.9239 - val_loss: 9.8712 - val_mae: 2.3038\n",
      "Epoch 431/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.7894 - mae: 0.9751 - val_loss: 13.5439 - val_mae: 2.9338\n",
      "Epoch 432/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.7145 - mae: 0.9446 - val_loss: 12.4530 - val_mae: 2.5886\n",
      "Epoch 433/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 2.0096 - mae: 1.0105 - val_loss: 10.6108 - val_mae: 2.5452\n",
      "Epoch 434/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5712 - mae: 0.9126 - val_loss: 10.6701 - val_mae: 2.4048\n",
      "Epoch 435/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.6941 - mae: 0.9714 - val_loss: 10.9078 - val_mae: 2.5573\n",
      "Epoch 436/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.6987 - mae: 0.9407 - val_loss: 10.7096 - val_mae: 2.5092\n",
      "Epoch 437/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.7155 - mae: 0.9738 - val_loss: 12.7265 - val_mae: 2.7834\n",
      "Epoch 438/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.6782 - mae: 0.9415 - val_loss: 12.9684 - val_mae: 2.8038\n",
      "Epoch 439/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8533 - mae: 0.9657 - val_loss: 10.4736 - val_mae: 2.4946\n",
      "Epoch 440/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5751 - mae: 0.9066 - val_loss: 11.0781 - val_mae: 2.4874\n",
      "Epoch 441/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.9186 - mae: 1.0414 - val_loss: 11.0654 - val_mae: 2.5241\n",
      "Epoch 442/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.4670 - mae: 0.9011 - val_loss: 14.4405 - val_mae: 2.9769\n",
      "Epoch 443/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.7358 - mae: 1.0027 - val_loss: 12.6136 - val_mae: 2.6966\n",
      "Epoch 444/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5394 - mae: 0.9229 - val_loss: 12.2612 - val_mae: 2.6353\n",
      "Epoch 445/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.7151 - mae: 0.9765 - val_loss: 13.1917 - val_mae: 2.6365\n",
      "Epoch 446/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.7921 - mae: 0.9543 - val_loss: 12.6945 - val_mae: 2.7217\n",
      "Epoch 447/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.6496 - mae: 0.9419 - val_loss: 11.0167 - val_mae: 2.4913\n",
      "Epoch 448/600\n",
      "323/323 [==============================] - 0s 43us/sample - loss: 1.5955 - mae: 0.9254 - val_loss: 11.0291 - val_mae: 2.5644\n",
      "Epoch 449/600\n",
      "323/323 [==============================] - 0s 56us/sample - loss: 1.6415 - mae: 0.9463 - val_loss: 13.6833 - val_mae: 2.8749\n",
      "Epoch 450/600\n",
      "323/323 [==============================] - 0s 43us/sample - loss: 1.6360 - mae: 0.9122 - val_loss: 11.2973 - val_mae: 2.5933\n",
      "Epoch 451/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.6053 - mae: 0.9202 - val_loss: 10.5899 - val_mae: 2.5639\n",
      "Epoch 452/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5640 - mae: 0.9057 - val_loss: 15.6293 - val_mae: 3.0758\n",
      "Epoch 453/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.6827 - mae: 0.9748 - val_loss: 11.3569 - val_mae: 2.4956\n",
      "Epoch 454/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5267 - mae: 0.9103 - val_loss: 13.1031 - val_mae: 2.8439\n",
      "Epoch 455/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4744 - mae: 0.9221 - val_loss: 14.1894 - val_mae: 2.8827\n",
      "Epoch 456/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.6681 - mae: 0.9470 - val_loss: 11.2486 - val_mae: 2.5349\n",
      "Epoch 457/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5119 - mae: 0.8978 - val_loss: 11.5828 - val_mae: 2.7034\n",
      "Epoch 458/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.6119 - mae: 0.9623 - val_loss: 11.3982 - val_mae: 2.5298\n",
      "Epoch 459/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5771 - mae: 0.9238 - val_loss: 10.6438 - val_mae: 2.5531\n",
      "Epoch 460/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.7466 - mae: 0.9615 - val_loss: 10.5453 - val_mae: 2.4569\n",
      "Epoch 461/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5632 - mae: 0.9000 - val_loss: 11.1572 - val_mae: 2.5555\n",
      "Epoch 462/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4366 - mae: 0.8795 - val_loss: 11.0933 - val_mae: 2.4710\n",
      "Epoch 463/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5945 - mae: 0.9149 - val_loss: 9.8583 - val_mae: 2.3660\n",
      "Epoch 464/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.6039 - mae: 0.9310 - val_loss: 11.4304 - val_mae: 2.5367\n",
      "Epoch 465/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.7612 - mae: 0.9722 - val_loss: 13.2652 - val_mae: 2.9886\n",
      "Epoch 466/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8404 - mae: 0.9909 - val_loss: 10.4876 - val_mae: 2.5796\n",
      "Epoch 467/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4189 - mae: 0.8786 - val_loss: 10.6049 - val_mae: 2.5838\n",
      "Epoch 468/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.4629 - mae: 0.8878 - val_loss: 10.3037 - val_mae: 2.4793\n",
      "Epoch 469/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.4532 - mae: 0.9067 - val_loss: 11.5801 - val_mae: 2.5882\n",
      "Epoch 470/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5778 - mae: 0.9214 - val_loss: 11.1448 - val_mae: 2.5376\n",
      "Epoch 471/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.4822 - mae: 0.9051 - val_loss: 11.7527 - val_mae: 2.6247\n",
      "Epoch 472/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 1.7030 - mae: 0.9815 - val_loss: 12.3600 - val_mae: 2.6017\n",
      "Epoch 473/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5550 - mae: 0.9161 - val_loss: 12.1261 - val_mae: 2.8614\n",
      "Epoch 474/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.7831 - mae: 1.0022 - val_loss: 10.0598 - val_mae: 2.4390\n",
      "Epoch 475/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.4220 - mae: 0.8791 - val_loss: 10.9532 - val_mae: 2.4957\n",
      "Epoch 476/600\n",
      "323/323 [==============================] - 0s 71us/sample - loss: 1.4628 - mae: 0.8743 - val_loss: 12.3152 - val_mae: 2.7563\n",
      "Epoch 477/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 1.4285 - mae: 0.8755 - val_loss: 11.4365 - val_mae: 2.5173\n",
      "Epoch 478/600\n",
      "323/323 [==============================] - 0s 43us/sample - loss: 1.6605 - mae: 0.9652 - val_loss: 10.7961 - val_mae: 2.5096\n",
      "Epoch 479/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.6176 - mae: 0.9255 - val_loss: 10.9473 - val_mae: 2.5843\n",
      "Epoch 480/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4072 - mae: 0.8904 - val_loss: 10.6615 - val_mae: 2.6056\n",
      "Epoch 481/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5085 - mae: 0.8845 - val_loss: 10.2255 - val_mae: 2.4667\n",
      "Epoch 482/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4984 - mae: 0.9018 - val_loss: 12.1418 - val_mae: 2.6647\n",
      "Epoch 483/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3642 - mae: 0.8888 - val_loss: 14.3074 - val_mae: 2.9232\n",
      "Epoch 484/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.6084 - mae: 0.9342 - val_loss: 10.7380 - val_mae: 2.5822\n",
      "Epoch 485/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5459 - mae: 0.9171 - val_loss: 11.6616 - val_mae: 2.5318\n",
      "Epoch 486/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 1.5687 - mae: 0.9158 - val_loss: 10.3654 - val_mae: 2.4861\n",
      "Epoch 487/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4469 - mae: 0.9133 - val_loss: 13.8477 - val_mae: 2.9593\n",
      "Epoch 488/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5743 - mae: 0.9466 - val_loss: 11.2683 - val_mae: 2.5271\n",
      "Epoch 489/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.3532 - mae: 0.8560 - val_loss: 10.5829 - val_mae: 2.5250\n",
      "Epoch 490/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.4782 - mae: 0.9087 - val_loss: 11.9013 - val_mae: 2.7833\n",
      "Epoch 491/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4334 - mae: 0.8915 - val_loss: 10.4286 - val_mae: 2.4359\n",
      "Epoch 492/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5806 - mae: 0.9255 - val_loss: 11.1319 - val_mae: 2.5802\n",
      "Epoch 493/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.3202 - mae: 0.8406 - val_loss: 11.5604 - val_mae: 2.6963\n",
      "Epoch 494/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4974 - mae: 0.9106 - val_loss: 15.6707 - val_mae: 3.0356\n",
      "Epoch 495/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.6430 - mae: 0.9581 - val_loss: 11.5665 - val_mae: 2.5753\n",
      "Epoch 496/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4096 - mae: 0.8715 - val_loss: 12.0125 - val_mae: 2.5667\n",
      "Epoch 497/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5658 - mae: 0.9337 - val_loss: 10.9972 - val_mae: 2.6289\n",
      "Epoch 498/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4543 - mae: 0.8792 - val_loss: 12.3995 - val_mae: 2.7125\n",
      "Epoch 499/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.4821 - mae: 0.9155 - val_loss: 15.0815 - val_mae: 3.0415\n",
      "Epoch 500/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.7167 - mae: 0.9664 - val_loss: 11.9948 - val_mae: 2.5878\n",
      "Epoch 501/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.4431 - mae: 0.9064 - val_loss: 10.5978 - val_mae: 2.6010\n",
      "Epoch 502/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.3689 - mae: 0.8682 - val_loss: 10.8538 - val_mae: 2.6802\n",
      "Epoch 503/600\n",
      "323/323 [==============================] - 0s 65us/sample - loss: 1.3653 - mae: 0.8569 - val_loss: 12.1458 - val_mae: 2.6166\n",
      "Epoch 504/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5437 - mae: 0.9329 - val_loss: 11.1467 - val_mae: 2.5646\n",
      "Epoch 505/600\n",
      "323/323 [==============================] - 0s 43us/sample - loss: 1.3219 - mae: 0.8532 - val_loss: 11.1753 - val_mae: 2.5143\n",
      "Epoch 506/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5137 - mae: 0.8833 - val_loss: 13.5848 - val_mae: 2.9335\n",
      "Epoch 507/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3714 - mae: 0.8747 - val_loss: 10.2683 - val_mae: 2.4367\n",
      "Epoch 508/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5363 - mae: 0.9262 - val_loss: 13.0505 - val_mae: 2.7806\n",
      "Epoch 509/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4120 - mae: 0.8487 - val_loss: 12.4449 - val_mae: 2.8102\n",
      "Epoch 510/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3407 - mae: 0.8786 - val_loss: 10.7166 - val_mae: 2.4710\n",
      "Epoch 511/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4622 - mae: 0.9235 - val_loss: 12.1325 - val_mae: 2.5786\n",
      "Epoch 512/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3840 - mae: 0.8664 - val_loss: 10.7101 - val_mae: 2.5861\n",
      "Epoch 513/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.4319 - mae: 0.9002 - val_loss: 11.8275 - val_mae: 2.5855\n",
      "Epoch 514/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1968 - mae: 0.8109 - val_loss: 14.6542 - val_mae: 2.9736\n",
      "Epoch 515/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.8068 - mae: 1.0004 - val_loss: 11.0465 - val_mae: 2.5369\n",
      "Epoch 516/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.2645 - mae: 0.8214 - val_loss: 11.2023 - val_mae: 2.6339\n",
      "Epoch 517/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5377 - mae: 0.9309 - val_loss: 12.0347 - val_mae: 2.6638\n",
      "Epoch 518/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4147 - mae: 0.8730 - val_loss: 12.1104 - val_mae: 2.6268\n",
      "Epoch 519/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2829 - mae: 0.8686 - val_loss: 10.6030 - val_mae: 2.5390\n",
      "Epoch 520/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.3222 - mae: 0.8396 - val_loss: 12.1782 - val_mae: 2.6965\n",
      "Epoch 521/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.2590 - mae: 0.8291 - val_loss: 11.8319 - val_mae: 2.6046\n",
      "Epoch 522/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5771 - mae: 0.9516 - val_loss: 12.0501 - val_mae: 2.6566\n",
      "Epoch 523/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3991 - mae: 0.8423 - val_loss: 14.6170 - val_mae: 3.1252\n",
      "Epoch 524/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.5647 - mae: 0.8914 - val_loss: 11.5703 - val_mae: 2.6384\n",
      "Epoch 525/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2845 - mae: 0.8253 - val_loss: 11.2425 - val_mae: 2.5586\n",
      "Epoch 526/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1760 - mae: 0.7952 - val_loss: 15.1408 - val_mae: 2.9455\n",
      "Epoch 527/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.4173 - mae: 0.8661 - val_loss: 12.0435 - val_mae: 2.6844\n",
      "Epoch 528/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2991 - mae: 0.8345 - val_loss: 10.6189 - val_mae: 2.6399\n",
      "Epoch 529/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3946 - mae: 0.8872 - val_loss: 11.6246 - val_mae: 2.6274\n",
      "Epoch 530/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4469 - mae: 0.8897 - val_loss: 10.5080 - val_mae: 2.5073\n",
      "Epoch 531/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.1645 - mae: 0.7994 - val_loss: 11.7133 - val_mae: 2.8052\n",
      "Epoch 532/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4433 - mae: 0.8975 - val_loss: 10.2470 - val_mae: 2.4694\n",
      "Epoch 533/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2105 - mae: 0.8263 - val_loss: 10.2511 - val_mae: 2.4739\n",
      "Epoch 534/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.2831 - mae: 0.8553 - val_loss: 11.6839 - val_mae: 2.5860\n",
      "Epoch 535/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.3795 - mae: 0.8477 - val_loss: 11.2926 - val_mae: 2.6058\n",
      "Epoch 536/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 1.3351 - mae: 0.8776 - val_loss: 14.0006 - val_mae: 3.0644\n",
      "Epoch 537/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5706 - mae: 0.8945 - val_loss: 10.9250 - val_mae: 2.5334\n",
      "Epoch 538/600\n",
      "323/323 [==============================] - 0s 56us/sample - loss: 1.2461 - mae: 0.8272 - val_loss: 13.0439 - val_mae: 2.8547\n",
      "Epoch 539/600\n",
      "323/323 [==============================] - 0s 46us/sample - loss: 1.3337 - mae: 0.8548 - val_loss: 11.6770 - val_mae: 2.6459\n",
      "Epoch 540/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3138 - mae: 0.8589 - val_loss: 14.2587 - val_mae: 2.9529\n",
      "Epoch 541/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.2860 - mae: 0.8258 - val_loss: 11.2971 - val_mae: 2.6350\n",
      "Epoch 542/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2243 - mae: 0.7970 - val_loss: 12.1240 - val_mae: 2.7492\n",
      "Epoch 543/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 1.3711 - mae: 0.8837 - val_loss: 12.7696 - val_mae: 2.6791\n",
      "Epoch 544/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3437 - mae: 0.8464 - val_loss: 10.8295 - val_mae: 2.5068\n",
      "Epoch 545/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 1.2601 - mae: 0.8409 - val_loss: 10.4878 - val_mae: 2.4791\n",
      "Epoch 546/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 1.4313 - mae: 0.8805 - val_loss: 10.6616 - val_mae: 2.5517\n",
      "Epoch 547/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1851 - mae: 0.8115 - val_loss: 10.3054 - val_mae: 2.4339\n",
      "Epoch 548/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.1840 - mae: 0.8145 - val_loss: 11.2630 - val_mae: 2.6630\n",
      "Epoch 549/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.5043 - mae: 0.9238 - val_loss: 11.3335 - val_mae: 2.5723\n",
      "Epoch 550/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.3546 - mae: 0.8385 - val_loss: 12.5429 - val_mae: 2.8334\n",
      "Epoch 551/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2851 - mae: 0.8193 - val_loss: 12.1976 - val_mae: 2.7812\n",
      "Epoch 552/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2169 - mae: 0.8306 - val_loss: 10.7985 - val_mae: 2.4394\n",
      "Epoch 553/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.2268 - mae: 0.8300 - val_loss: 10.9761 - val_mae: 2.4945\n",
      "Epoch 554/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.3180 - mae: 0.8324 - val_loss: 11.2206 - val_mae: 2.5623\n",
      "Epoch 555/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.3033 - mae: 0.8517 - val_loss: 11.0345 - val_mae: 2.6549\n",
      "Epoch 556/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2142 - mae: 0.7936 - val_loss: 11.0423 - val_mae: 2.7352\n",
      "Epoch 557/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.3520 - mae: 0.8673 - val_loss: 12.6055 - val_mae: 2.6268\n",
      "Epoch 558/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.3357 - mae: 0.8293 - val_loss: 10.9471 - val_mae: 2.5955\n",
      "Epoch 559/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1860 - mae: 0.7933 - val_loss: 12.0828 - val_mae: 2.5852\n",
      "Epoch 560/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.6234 - mae: 0.9379 - val_loss: 10.7542 - val_mae: 2.5082\n",
      "Epoch 561/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1662 - mae: 0.7649 - val_loss: 10.6219 - val_mae: 2.5575\n",
      "Epoch 562/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3379 - mae: 0.8544 - val_loss: 10.8488 - val_mae: 2.5388\n",
      "Epoch 563/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.2320 - mae: 0.8193 - val_loss: 10.1446 - val_mae: 2.4363\n",
      "Epoch 564/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3102 - mae: 0.8599 - val_loss: 12.5180 - val_mae: 2.6149\n",
      "Epoch 565/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.4657 - mae: 0.8713 - val_loss: 11.6250 - val_mae: 2.6653\n",
      "Epoch 566/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.2767 - mae: 0.8319 - val_loss: 10.5735 - val_mae: 2.4985\n",
      "Epoch 567/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2433 - mae: 0.8356 - val_loss: 11.4890 - val_mae: 2.6662\n",
      "Epoch 568/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1750 - mae: 0.8178 - val_loss: 11.4309 - val_mae: 2.6946\n",
      "Epoch 569/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2767 - mae: 0.8192 - val_loss: 11.2093 - val_mae: 2.4712\n",
      "Epoch 570/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.2386 - mae: 0.8306 - val_loss: 10.7410 - val_mae: 2.5325\n",
      "Epoch 571/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1168 - mae: 0.7796 - val_loss: 11.1519 - val_mae: 2.6249\n",
      "Epoch 572/600\n",
      "323/323 [==============================] - 0s 68us/sample - loss: 1.4206 - mae: 0.8790 - val_loss: 11.0424 - val_mae: 2.5201\n",
      "Epoch 573/600\n",
      "323/323 [==============================] - 0s 43us/sample - loss: 1.1977 - mae: 0.8104 - val_loss: 11.4798 - val_mae: 2.6132\n",
      "Epoch 574/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 1.1764 - mae: 0.8165 - val_loss: 11.3350 - val_mae: 2.5944\n",
      "Epoch 575/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1599 - mae: 0.7900 - val_loss: 11.1285 - val_mae: 2.4703\n",
      "Epoch 576/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4240 - mae: 0.8758 - val_loss: 11.0789 - val_mae: 2.5542\n",
      "Epoch 577/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2267 - mae: 0.8189 - val_loss: 11.0939 - val_mae: 2.6146\n",
      "Epoch 578/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1566 - mae: 0.8094 - val_loss: 11.1648 - val_mae: 2.5467\n",
      "Epoch 579/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.0510 - mae: 0.7350 - val_loss: 14.4239 - val_mae: 3.0861\n",
      "Epoch 580/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4220 - mae: 0.8992 - val_loss: 11.4284 - val_mae: 2.6701\n",
      "Epoch 581/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3463 - mae: 0.8807 - val_loss: 12.4257 - val_mae: 2.7276\n",
      "Epoch 582/600\n",
      "323/323 [==============================] - 0s 40us/sample - loss: 1.1716 - mae: 0.7799 - val_loss: 11.2035 - val_mae: 2.7078\n",
      "Epoch 583/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1045 - mae: 0.8050 - val_loss: 12.1166 - val_mae: 2.5936\n",
      "Epoch 584/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.4282 - mae: 0.8755 - val_loss: 11.2102 - val_mae: 2.5761\n",
      "Epoch 585/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1109 - mae: 0.7829 - val_loss: 12.8614 - val_mae: 2.6742\n",
      "Epoch 586/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1805 - mae: 0.8240 - val_loss: 11.7997 - val_mae: 2.5627\n",
      "Epoch 587/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3455 - mae: 0.8422 - val_loss: 10.6641 - val_mae: 2.5273\n",
      "Epoch 588/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.1271 - mae: 0.7720 - val_loss: 10.8523 - val_mae: 2.4802\n",
      "Epoch 589/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1701 - mae: 0.7892 - val_loss: 11.0466 - val_mae: 2.6046\n",
      "Epoch 590/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.1498 - mae: 0.7975 - val_loss: 13.9306 - val_mae: 2.8516\n",
      "Epoch 591/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3525 - mae: 0.8573 - val_loss: 10.5971 - val_mae: 2.5135\n",
      "Epoch 592/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.0688 - mae: 0.7344 - val_loss: 12.3602 - val_mae: 2.8096\n",
      "Epoch 593/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2447 - mae: 0.8174 - val_loss: 11.4223 - val_mae: 2.6611\n",
      "Epoch 594/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.1037 - mae: 0.7598 - val_loss: 13.2108 - val_mae: 2.8636\n",
      "Epoch 595/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2809 - mae: 0.8148 - val_loss: 10.7656 - val_mae: 2.5820\n",
      "Epoch 596/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.2133 - mae: 0.8094 - val_loss: 15.5791 - val_mae: 3.1812\n",
      "Epoch 597/600\n",
      "323/323 [==============================] - 0s 37us/sample - loss: 1.3673 - mae: 0.8207 - val_loss: 11.2400 - val_mae: 2.6707\n",
      "Epoch 598/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.0703 - mae: 0.7529 - val_loss: 11.6682 - val_mae: 2.6395\n",
      "Epoch 599/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 0.9692 - mae: 0.7288 - val_loss: 10.7015 - val_mae: 2.5562\n",
      "Epoch 600/600\n",
      "323/323 [==============================] - 0s 34us/sample - loss: 1.2516 - mae: 0.8464 - val_loss: 11.2547 - val_mae: 2.5480\n"
     ]
    }
   ],
   "source": [
    "# Treinamento da rede\n",
    "history = model.fit(train_data, train_labels, epochs=600, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Função para a criação do gráfico do erro médio absoluto e do erro médio quadrático\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error [$1000]')\n",
    "\n",
    "    plt.plot(history.epoch, numpy.array(history.history['mae']), 'k-', label='Mean Absolute Error',)\n",
    "    plt.plot(history.epoch, numpy.array(history.history['val_mae']), 'r-' , label='Validation MAE')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Erro médio absoluto da rede neural')\n",
    "    plt.ylim([0, 5])\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O erro médio absoluto em relação aos originais preços da casa é: $2246.23\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([28.2, 23.9, 16.6, 22. , 20.8, 23. , 27.9, 14.5, 21.5, 22.6, 23.7,\n",
       "       31.2, 19.3, 19.4, 19.4, 27.9, 13.9, 50. , 24.1, 14.6, 16.2, 15.6,\n",
       "       23.8, 25. , 23.5,  8.3, 13.5, 17.5, 43.1, 11.5, 24.1, 18.5, 50. ,\n",
       "       12.6, 19.8, 24.5, 14.9, 36.2, 11.9, 19.1, 22.6, 20.7, 30.1, 13.3,\n",
       "       14.6,  8.4, 50. , 12.7, 25. , 18.6, 29.8, 22.2, 28.7, 23.8,  8.1,\n",
       "       22.2,  6.3, 22.1, 17.5, 48.3, 16.7, 26.6,  8.5, 14.5, 23.7, 37.2,\n",
       "       41.7, 16.5, 21.7, 22.7, 23. , 10.5, 21.9, 21. , 20.4, 21.8, 50. ,\n",
       "       22. , 23.3, 37.3, 18. , 19.2, 34.9, 13.4, 22.9, 22.5, 13. , 24.6,\n",
       "       18.3, 18.1, 23.9, 50. , 13.6, 22.9, 10.9, 18.9, 22.4, 22.9, 44.8,\n",
       "       21.7, 10.2, 15.4, 25.3, 23.3,  7.2, 21.2, 11.7, 27. , 29.6, 26.5,\n",
       "       43.5, 23.6, 11. , 33.4, 36. , 36.4, 19. , 20.2, 34.9, 50. , 19.3,\n",
       "       14.9, 26.6, 19.9, 24.8, 21.2, 23.9, 20.6, 23.1, 28. , 20. , 23.1,\n",
       "       25. ,  9.7, 23.9, 36.1, 13.4, 12.7, 39.8, 10.4, 20.6, 17.8, 19.5,\n",
       "       23.7, 28.5, 24.3, 23.8, 19.1, 28.4, 20.5, 33.8, 14.5, 20.4, 16. ,\n",
       "       13.3, 30.8, 27.5, 24.4, 24.4, 25.1, 43.8, 21.9, 26.2, 14.2, 20.8,\n",
       "       20.1, 23.1, 13.1, 16.2, 24.8, 20.2, 22.5, 14.8, 28.7, 20.1, 23.4,\n",
       "       32. , 19.1, 50. , 20.9, 21.7, 22. , 17.2, 30.3, 12.3, 21.4, 20.5,\n",
       "       35.2, 19.6, 22. , 21.7, 14.1, 21.1, 15. , 11.9, 20. , 41.3, 18.7,\n",
       "       50. , 50. , 18.4, 17.9, 28.1, 16.1, 17.2, 28.6, 23.6, 20.4, 19.6,\n",
       "       18.8, 22.6, 17.7, 30.5, 18.2, 20.6, 24.4, 17.3, 13.3, 22.8, 20.5,\n",
       "       21.2, 18.8, 18.9, 18.2, 23.1, 32.7, 24. , 10.2, 19.5, 33.1, 13.4,\n",
       "       15.2, 24.8, 24.3,  9.5, 24.2, 18.5, 44. , 50. , 24.7, 21.5,  8.4,\n",
       "       21.8, 50. , 23.8, 32.4, 24.4, 17.6, 29.8,  9.6, 16.7, 13.8, 32. ,\n",
       "       16.1,  8.3, 26.6, 14.3, 15. , 28.4, 32.2, 17.1, 29.4, 10.4, 16.8,\n",
       "       31.5, 27.5, 46.7, 27.5, 17.2, 23.4, 31.6, 13.8, 22. , 17. , 24.8,\n",
       "       24.3, 25.2, 21.2, 20.6, 18.7,  5.6, 19.3, 19.8, 22.3, 20.3, 12. ,\n",
       "       23.9, 16.5, 13.2, 33.2, 10.5,  7.5, 27.5, 18.4, 23.2, 13.8, 35.4,\n",
       "       23. , 25. ,  7.2, 14.4,  8.8, 22.7, 13.1, 18.9, 25. ,  8.5, 16.1,\n",
       "       29. , 23.1, 19.3, 33.1, 24.6, 23. , 15.2, 27.1, 19.6, 24.5, 20.3,\n",
       "       34.9, 17.1, 15.6, 26.4, 22.6, 15.6, 29. , 21.2, 22.4, 13.5, 11.7,\n",
       "       17.1, 31.7, 28.7, 24.7, 19. ,  7.2, 13.8, 12.8, 36.2, 38.7, 18.5,\n",
       "       29.1, 20.4, 11.3, 17.4,  8.7, 18.9, 23.2, 22.2, 29.1, 34.6, 25. ,\n",
       "       23.2, 37.9,  7. , 18.2, 19.3, 26.7, 19.2, 30.1, 20.6, 50. , 18.7,\n",
       "       20.6, 31.1, 14. , 17.8, 42.3, 15.3, 18.5, 21.4, 15. , 20.7, 21.4,\n",
       "       21.7, 22. , 31.6, 22. , 10.2, 22.6, 20. , 17.8, 13.6, 11.8, 19.4,\n",
       "       21.4, 32.9, 20.8, 31. , 17.5, 15.4, 10.8, 34.7, 25. , 48.8, 42.8,\n",
       "       19.5, 30.1, 22.2, 50. , 23.1, 32.5, 19.6, 14.9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = model.evaluate(test_data, test_labels, verbose=0)[1]\n",
    "print(\"O erro médio absoluto em relação aos originais preços da casa é: ${:5.2f}\".format(mae * 1000))\n",
    "\n",
    "predictions = model.predict(test_data).flatten()\n",
    "plt.subplots(figsize=(5, 5))\n",
    "plt.scatter(test_labels, predictions)\n",
    "plt.xlabel('True values [$1000]')\n",
    "plt.ylabel('Predictions [$1000]')\n",
    "plt.title('Comparação entre os valores originais e os escolhidos pela rede neural')\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21c4bb3b271cdc712da94708afa9df2938311afcda944364ee0384b6a9c3a282"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
